{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\feder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\feder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import nltk\n",
    "import string\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"data.json\")\n",
    "data_out= pd.read_json(\"data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4224"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4224 entries, 0 to 4223\n",
      "Data columns (total 7 columns):\n",
      "created_at        4224 non-null datetime64[ns]\n",
      "favorite_count    4224 non-null int64\n",
      "id_str            4224 non-null int64\n",
      "is_retweet        4224 non-null bool\n",
      "retweet_count     4224 non-null int64\n",
      "source            4224 non-null object\n",
      "text              4224 non-null object\n",
      "dtypes: bool(1), datetime64[ns](1), int64(3), object(2)\n",
      "memory usage: 202.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone','user',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"elongated\", \"repeated\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated word\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=False).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "normalized_text = []\n",
    "\n",
    "for s in data['text']:\n",
    "    normalized_text.append(\" \".join(text_processor.pre_process_doc(s)))\n",
    "normalized_text_series = pd.Series(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['normalized_text'] = [t for t in normalized_text_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"created_at\",\"favorite_count\",\"id_str\",\"is_retweet\",\"source\",\"retweet_count\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "notag_tweets = []\n",
    "for t in data.normalized_text:\n",
    "    notag_tweets.append(re.sub(r'(<[aA-zZ]+>)', '',t))\n",
    "\n",
    "data['text_no_tag'] = [t for t in notag_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nopun_tweets = []\n",
    "for t in data.text_no_tag:\n",
    "    nopun_tweets.append(re.sub(r'[^\\w\\s]', '',t))\n",
    "\n",
    "data['text_no_pun'] = [t for t in nopun_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>text_no_tag</th>\n",
       "      <th>text_no_pun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russians are playing @CNN and @NBCNews for suc...</td>\n",
       "      <td>Russians are playing &lt;user&gt; and &lt;user&gt; for suc...</td>\n",
       "      <td>Russians are playing  and  for such fools - fu...</td>\n",
       "      <td>Russians are playing  and  for such fools  fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join @AmerIcan32, founded by Hall of Fame lege...</td>\n",
       "      <td>Join &lt;user&gt; , founded by Hall of Fame legend &lt;...</td>\n",
       "      <td>Join  , founded by Hall of Fame legend  on  in...</td>\n",
       "      <td>Join   founded by Hall of Fame legend  on  in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great move on delay (by V. Putin) - I always k...</td>\n",
       "      <td>Great move on delay ( by V . Putin ) - I alway...</td>\n",
       "      <td>Great move on delay ( by V . Putin ) - I alway...</td>\n",
       "      <td>Great move on delay  by V  Putin   I always kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My Administration will follow two simple rules...</td>\n",
       "      <td>My Administration will follow two simple rules...</td>\n",
       "      <td>My Administration will follow two simple rules :</td>\n",
       "      <td>My Administration will follow two simple rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Economists say Trump delivered hope' https://...</td>\n",
       "      <td>' Economists say Trump delivered hope ' &lt;url&gt;</td>\n",
       "      <td>' Economists say Trump delivered hope '</td>\n",
       "      <td>Economists say Trump delivered hope</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Russians are playing @CNN and @NBCNews for suc...   \n",
       "1  Join @AmerIcan32, founded by Hall of Fame lege...   \n",
       "2  Great move on delay (by V. Putin) - I always k...   \n",
       "3  My Administration will follow two simple rules...   \n",
       "4  'Economists say Trump delivered hope' https://...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  Russians are playing <user> and <user> for suc...   \n",
       "1  Join <user> , founded by Hall of Fame legend <...   \n",
       "2  Great move on delay ( by V . Putin ) - I alway...   \n",
       "3  My Administration will follow two simple rules...   \n",
       "4      ' Economists say Trump delivered hope ' <url>   \n",
       "\n",
       "                                         text_no_tag  \\\n",
       "0  Russians are playing  and  for such fools - fu...   \n",
       "1  Join  , founded by Hall of Fame legend  on  in...   \n",
       "2  Great move on delay ( by V . Putin ) - I alway...   \n",
       "3  My Administration will follow two simple rules :    \n",
       "4           ' Economists say Trump delivered hope '    \n",
       "\n",
       "                                         text_no_pun  \n",
       "0  Russians are playing  and  for such fools  fun...  \n",
       "1  Join   founded by Hall of Fame legend  on  in ...  \n",
       "2  Great move on delay  by V  Putin   I always kn...  \n",
       "3   My Administration will follow two simple rules    \n",
       "4              Economists say Trump delivered hope    "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>text_no_tag</th>\n",
       "      <th>text_no_pun</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russians are playing @CNN and @NBCNews for suc...</td>\n",
       "      <td>Russians are playing &lt;user&gt; and &lt;user&gt; for suc...</td>\n",
       "      <td>Russians are playing  and  for such fools - fu...</td>\n",
       "      <td>Russians are playing  and  for such fools  fun...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join @AmerIcan32, founded by Hall of Fame lege...</td>\n",
       "      <td>Join &lt;user&gt; , founded by Hall of Fame legend &lt;...</td>\n",
       "      <td>Join  , founded by Hall of Fame legend  on  in...</td>\n",
       "      <td>Join   founded by Hall of Fame legend  on  in ...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great move on delay (by V. Putin) - I always k...</td>\n",
       "      <td>Great move on delay ( by V . Putin ) - I alway...</td>\n",
       "      <td>Great move on delay ( by V . Putin ) - I alway...</td>\n",
       "      <td>Great move on delay  by V  Putin   I always kn...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My Administration will follow two simple rules...</td>\n",
       "      <td>My Administration will follow two simple rules...</td>\n",
       "      <td>My Administration will follow two simple rules :</td>\n",
       "      <td>My Administration will follow two simple rules</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Economists say Trump delivered hope' https://...</td>\n",
       "      <td>' Economists say Trump delivered hope ' &lt;url&gt;</td>\n",
       "      <td>' Economists say Trump delivered hope '</td>\n",
       "      <td>Economists say Trump delivered hope</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Russians are playing @CNN and @NBCNews for suc...   \n",
       "1  Join @AmerIcan32, founded by Hall of Fame lege...   \n",
       "2  Great move on delay (by V. Putin) - I always k...   \n",
       "3  My Administration will follow two simple rules...   \n",
       "4  'Economists say Trump delivered hope' https://...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  Russians are playing <user> and <user> for suc...   \n",
       "1  Join <user> , founded by Hall of Fame legend <...   \n",
       "2  Great move on delay ( by V . Putin ) - I alway...   \n",
       "3  My Administration will follow two simple rules...   \n",
       "4      ' Economists say Trump delivered hope ' <url>   \n",
       "\n",
       "                                         text_no_tag  \\\n",
       "0  Russians are playing  and  for such fools - fu...   \n",
       "1  Join  , founded by Hall of Fame legend  on  in...   \n",
       "2  Great move on delay ( by V . Putin ) - I alway...   \n",
       "3  My Administration will follow two simple rules :    \n",
       "4           ' Economists say Trump delivered hope '    \n",
       "\n",
       "                                         text_no_pun  length  \n",
       "0  Russians are playing  and  for such fools  fun...     105  \n",
       "1  Join   founded by Hall of Fame legend  on  in ...      71  \n",
       "2  Great move on delay  by V  Putin   I always kn...      72  \n",
       "3   My Administration will follow two simple rules        49  \n",
       "4              Economists say Trump delivered hope        40  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['length'] = [len(t) for t in data.text_no_tag]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text_lower\"] = data.text_no_pun.str.lower().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data= data[\"text_lower\"]\n",
    "sw = stopwords.words('english')\n",
    "#words.add('putin') #change this to get more stuff to keep\n",
    "\n",
    "l = nltk.stem.WordNetLemmatizer()\n",
    "#test_tweet=x_data[1]\n",
    "\n",
    "\n",
    "def remove_stop_lemmatiz(tweet):\n",
    "    part= [word for word in tweet if word not in stopwords.words('english')]\n",
    "    part=[l.lemmatize(word) for word in part] #comment out if not want to remove \"no english words\". \n",
    "    #part= [word for word in part if word in words]\n",
    "    return part \n",
    "\n",
    "def remove_custom_words(tweet,wordlist):\n",
    "    return [word for word in tweet if word in wordlist]\n",
    "\n",
    "\n",
    "\n",
    "x_data_cleaned=[remove_stop_lemmatiz(tweet) for tweet in x_data ]  \n",
    "superstring=''\n",
    "\n",
    "for i in range(len(x_data_cleaned)):\n",
    "    superstring=superstring+' '.join(x_data_cleaned[i])+' '\n",
    "\n",
    "\n",
    "words_list= Counter(superstring.split()).most_common()\n",
    "words_to_use=set(Counter(superstring.split()).most_common())\n",
    "\n",
    "for i in range(len(words_list)):\n",
    "    if(words_list[i][1]>150 or words_list[i][1]<2):\n",
    "        words_to_use.remove(words_list[i])\n",
    "sorted_by_second = sorted(words_to_use, key=lambda tup: -tup[1])        \n",
    "final_word_allowed_set=set([i[0] for i in sorted_by_second])\n",
    "\n",
    "x_data_cleaned_2=[remove_custom_words(tweet,final_word_allowed_set) for tweet in x_data_cleaned]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=[' '.join(tweet) for tweet in x_data_cleaned_2]\n",
    "df_in=pd.DataFrame(new)\n",
    "df_in.columns=['text_pp']\n",
    "b=pd.concat((data_out,df_in), axis=1)\n",
    "b.to_csv('preprocess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
